// TODO: Move to <header> . <mem block> model vs separate struct
deftype Arena = {
  basePtr: Pointer,
  curAddr: u64,
  maxAddr: u64
}
ns Arena {
  let const mb: u64 = 0x100000;
  let const gb: u64 = 0x40000000;

  fn regionSizeBytes(self: Arena): u64 {
    self.maxAddr - (self.basePtr as u64)
  }

  // Aligns 'base' to the next available value that is aligned with `align`
  fn alignTo(baseAddress: u64, alignBytes: u64): u64 {
    let mask = alignBytes - 1;
    let sum = baseAddress + mask;
    let aligned = Bitwise/bitAnd(sum, Bitwise/bitNot(mask));
    #if k1/DEBUG if aligned != baseAddress {
      println("Aligned \{baseAddress} to \{aligned}!")
    };
    aligned
  }

  fn new(initialMb: u64): Arena {
    let capacity = initialMb * mb;
    // We zero the memory because we don't make arenas that often
    // so it's worth it
    let basePtr = mem/systemZeroAlloc(
      size = capacity, 
      align = 8
    );
    if basePtr.isNull() crash("calloc for Arena failed");
    {
      basePtr: basePtr,
      curAddr: basePtr as u64,
      maxAddr: basePtr as u64 + capacity
    }
  }

  fn pushRaw(self: Arena*, size: u64, align: u64): Pointer {
    let dataStart = alignTo(self.curAddr, align);
    let newEnd = dataStart + size;
    if newEnd > self.maxAddr {
      crash("Arena is out of space to allocate \{size} bytes.")
    };

    self.curAddr* <- newEnd;
    dataStart as Pointer
  }

  fn push[T](self: Arena*, t: T): T* {
    let dataStart = self.pushRaw(sizeOf[T](), alignOf[T]());

    let ref: T* = dataStart as T*;
    ref <- t;
    ref
  }

  fn freeAll(self: Arena*): unit {
    let size = self.regionSizeBytes();
    println("[arena] Freeing \{size}");
    libc/memset(self.basePtr, 0 as u32, size);
    self.curAddr* <- (self.basePtr as u64);
  }
}
